---
title: KD-ACR
summary: Improve Chord Recognition Algorithm by Knowledge Distilling
tags:
  - Music Information Retrieval
date: '2022-01-01T00:00:00Z'

# Optional external URL for project (replaces project detail page).
external_link: ''

image:
  caption: 
  focal_point: 

links:
  - icon: 
    icon_pack: 
    name: 
    url: 
url_code: ''
url_pdf: ''
url_slides: ''
url_video: ''

# Slides (optional).
#   Associate this project with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
slides: 
---

Description: Knowledge Distilling (KD) was first introduced to Automatic Chord Recognition (ACR) problem to compress model and get better real-time performance and accuracy. Besides, the influence of different networks and loss functions on the result was also explored in the project. Currently, Iâ€™m trying some Reinforcement Learning (RL) methods for the optimization of KD.

Research Production: Paper: Zijian Zhao: KD-ACR: Knowledge Distilling for Automatic Chord Recognition Model (under revision, first submitted on January 31st 2023 to IEEE Access)
